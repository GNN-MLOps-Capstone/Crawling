{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bafccff",
   "metadata": {},
   "source": [
    "깔아야 하는 라이브러리\n",
    "[ollama pandas tqdm konlpy scikit-learn numpy hanja kiwipiepy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499ba7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import hanja\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "from kiwipiepy import Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd6c896",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'crawled_news_20251005_174551.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m news_file_name = \u001b[33m'\u001b[39m\u001b[33mcrawled_news_20251005_174551.xlsx\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_news = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_news=df_news.loc[:,[\u001b[33m'\u001b[39m\u001b[33mcrawled_news_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mpub_date\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/pandas/io/excel/_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/pandas/io/excel/_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.13/dist-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'crawled_news_20251005_174551.xlsx'"
     ]
    }
   ],
   "source": [
    "news_file_name = 'crawled_news_20251005_174551.xlsx'\n",
    "df_news = pd.read_excel(news_file_name)\n",
    "df_news=df_news.loc[:,['crawled_news_id','title','pub_date','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3090c6d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_news\u001b[49m.shape)\n\u001b[32m      2\u001b[39m df_news.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_news' is not defined"
     ]
    }
   ],
   "source": [
    "print(df_news.shape)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa688906",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m yesterday = today - timedelta(days=\u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 만약에 전체 기사로 결과를 보고 싶다면 아래 코드 한줄을 주석 처리\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#df_news = df_news[df_news['pub_date'].dt.date == yesterday]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(yesterday,\u001b[33m\"\u001b[39m\u001b[33m의 뉴스 개수 : \u001b[39m\u001b[33m\"\u001b[39m,\u001b[43mdf_news\u001b[49m.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     10\u001b[39m df_news.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_news' is not defined"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "# 이건 임의로 어제 기사가 없으므로(나중에 실사용 할때에는 지우기!!!)\n",
    "today = date(2025, 10, 1)\n",
    "\n",
    "yesterday = today - timedelta(days=1)\n",
    "\n",
    "# 만약에 전체 기사로 결과를 보고 싶다면 아래 코드 한줄을 주석 처리\n",
    "#df_news = df_news[df_news['pub_date'].dt.date == yesterday]\n",
    "print(yesterday,\"의 뉴스 개수 : \",df_news.shape[0])\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "207ae16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거로 삭제된 행 :  175\n"
     ]
    }
   ],
   "source": [
    "df_news_dup = df_news.copy()\n",
    "\n",
    "# 중복제거\n",
    "kr_news = df_news_dup.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "removed_duplicates = df_news_dup[df_news_dup.duplicated(subset=['text'], keep='first')].copy()\n",
    "removed_duplicates[\"reason\"] = \"중복\"\n",
    "\n",
    "print(\"중복 제거로 삭제된 행 : \", removed_duplicates.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdccf5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan값 제거로 삭제된 행 :  0\n"
     ]
    }
   ],
   "source": [
    "removed_nan = kr_news[kr_news.isnull().any(axis=1)].copy()\n",
    "removed_nan[\"reason\"] = \"nan값\"\n",
    "\n",
    "# nan값 제거 실행\n",
    "kr_news = kr_news.dropna()\n",
    "\n",
    "\n",
    "print(\"nan값 제거로 삭제된 행 : \",removed_nan.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16d5ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잘못된 본문이 삭제된 행 개수:  9\n"
     ]
    }
   ],
   "source": [
    "# 제거할 문구 목록\n",
    "remove_patterns = [\n",
    "    '해당기사를 찾을 수 없습니다',\n",
    "    '인터넷 법률신문은 인터넷신문윤리강령 및 그 실천요강을 준수합니다',\n",
    "    '국가정보자원관리원 화재로 인하여 현재 임시로 보도자료만 제공됩니다',\n",
    "    'BEST 댓글 답글과 추천수를 합산하여 자동으로 노출됩니다',\n",
    "    'SNS 기사보내기 카카오스토리(으)로 기사보내기',\n",
    "    'SNS 기사보내기 카카오스토리로 기사보내기',\n",
    "    '슬로우레터는 뉴스를 더 열심히 읽고 구조와 맥락을 이해하기 위한 프로젝트입니다'\n",
    "]\n",
    "\n",
    "\n",
    "# 문구가 포함된 행 찾기\n",
    "mask = kr_news['text'].apply(lambda x: any(pat in str(x) for pat in remove_patterns))\n",
    "removed_pattern = kr_news[mask].copy()\n",
    "removed_pattern[\"reason\"] = \"잘못된 본문\"\n",
    "kr_news = kr_news[~mask]\n",
    "\n",
    "print(\"잘못된 본문이 삭제된 행 개수: \",removed_pattern.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74e0c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목 정제\n",
    "kr_news['title'] = kr_news['title'].apply(lambda x: html.unescape(str(x)))\n",
    "kr_news['title'] = kr_news['title'].apply(lambda x: re.sub(r'[^A-Za-z0-9가-힇.,\\\"\\'\\:\\·\\!\\?\\-\\%\\~\\&]', ' ', x))\n",
    "kr_news['title'] = kr_news['title'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "936642ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목-본문 매칭이 되지 않아 삭제된 행 개수 :  74\n"
     ]
    }
   ],
   "source": [
    "#kiwi라는 형태소 분석 (제목 분석할 때 0kt보다 더 잘됨)\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 제목에서 명사 찾기\n",
    "def check_noun_title(row, kiwi_analyzer, threshold=0.5):\n",
    "    title = str(row['title'])\n",
    "    text = str(row['text'])\n",
    "\n",
    "    result = kiwi_analyzer.analyze(title)\n",
    "    \n",
    "    if not result:\n",
    "        return (True, [], 1.0, [])\n",
    "    \n",
    "    # 제목을 (단어, 품사)로 쪼개기\n",
    "    tagged_words = result[0][0]\n",
    "    \n",
    "    # 명사(NNG), 고유명사(NNP), SL(외국어, 알파벳)만 남김\n",
    "    title_nouns = [word for word, tag, _, _ in tagged_words \n",
    "                   if tag in ['NNG', 'NNP', 'SL']]\n",
    "    \n",
    "    if not title_nouns:\n",
    "        return (True, [], 1.0, tagged_words)\n",
    "\n",
    "    # 그 명사가 본문에 몇개 들어있는지 확인    \n",
    "    match_count = 0\n",
    "    for noun in title_nouns:\n",
    "        if noun in text:\n",
    "            match_count += 1\n",
    "            \n",
    "    # 제목에서 뽑아온 명사들이 본문에 어떤 비율로 들어있는지\n",
    "    match_ratio = match_count / len(title_nouns)\n",
    "    \n",
    "    return (match_ratio >= threshold, title_nouns, match_ratio, tagged_words)\n",
    "\n",
    "\n",
    "if kiwi: \n",
    "    current_threshold = 0.3\n",
    "\n",
    "    results_series = kr_news.apply(\n",
    "        check_noun_title,\n",
    "        axis=1,\n",
    "        kiwi_analyzer=kiwi, \n",
    "        threshold=current_threshold\n",
    "    )\n",
    "    # 기사 매칭이 0.3을 기준으로 true인지 false인지\n",
    "    mask = results_series.str[0]\n",
    "    \n",
    "    removed_mismatch = kr_news[~mask].copy()\n",
    "    removed_mismatch[\"reason\"] = \"제목, 본문 불일치\"\n",
    "    kr_news = kr_news[mask]\n",
    "\n",
    "    print(\"제목-본문 매칭이 되지 않아 삭제된 행 개수 : \", removed_mismatch.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b26016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 정제\n",
    "# HTML 이스케이프 해제 및 한자 변환\n",
    "kr_news['text'] = kr_news['text'].apply(lambda x: html.unescape(str(x)))\n",
    "kr_news['text'] = kr_news['text'].apply(lambda x: hanja.translate(x, 'substitution'))\n",
    "\n",
    "# 제거할 패턴들\n",
    "patterns_to_remove = [\n",
    "    r'\\([^)]*\\)|\\[[^\\]]*\\]|\\{[^}]*\\}|<[^>]*>',\n",
    "    r'\\S*@\\S*',\n",
    "    r'http\\S+|www\\S+',\n",
    "    r'\\S*=\\S*',\n",
    "    r'[가-힣]{2,4}\\s?(기자|특파원)',\n",
    "    r'(연합뉴스|뉴스1|뉴시스|조선일보|중앙일보|동아일보|한겨레|한국일보|서울경제|매일경제|머니투데이|한국경제|경향신문|헤럴드경제|아시아경제|이데일리|데일리안|세계일보|국민일보|뉴스핌|파이낸셜뉴스)',\n",
    "    r'(무단전재\\s*및\\s*재배포\\s*금지|저작권자[^.,\\n]+|Copyright\\s*ⓒ[^.,\\n]+|끝\\)|끝$)',\n",
    "    r'(사진\\s*=\\s*[^.,\\n]+|관련기사|주요뉴스|이 시각 뉴스)[^.\\n]*',\n",
    "    r'SNS\\s*기사보내기',\n",
    "    r'카카오톡로\\s*기사보내기',\n",
    "    r'URL복사로\\s*기사보내기',\n",
    "    r'다른\\s*공유\\s*찾기',\n",
    "    r'기사와\\s*상관\\s*없는\\s*자료사진',\n",
    "    r'fullscreen',\n",
    "    r'News1\\s*DB',\n",
    "    r'ENM\\s*본\\s*기사는\\s*스포일러를\\s*포함하고\\s*있습니다.',\n",
    "    r'photo',\n",
    "    r'일러스트\\s*NEWS\\s*IMAGE',\n",
    "    r'News1',\n",
    "    r'기사의\\s*본문\\s*내용은\\s*이\\s*글자크기로\\s*변경됩니다.',\n",
    "    r'아래\\s*텍스트는\\s*속기초안이며,\\s*추후\\s*업데이트\\s*됩니다.',\n",
    "    r'아래\\s*텍스트는\\s*실제\\s*방송\\s*내용과\\s*차이가\\s*있을\\s*수\\s*있으니\\s*보다\\s*정확한\\s*내용은\\s*방송으로\\s*확인하시기\\s*바랍니다.',\n",
    "    r'한강타임즈는\\s*언제나\\s*여러분의\\s*제보를\\s*기다립니다.',\n",
    "    r'※.*|▶.*|★.*'\n",
    "]\n",
    "\n",
    "for pattern in patterns_to_remove:\n",
    "    kr_news['text'] = kr_news['text'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "\n",
    "kr_news['text'] = kr_news['text'].apply(lambda x: re.sub(r'[^A-Za-z0-9가-힇.,\\\"\\'\\:\\·\\!\\?\\-\\%\\~\\&]', ' ', x))\n",
    "# 공백 정리\n",
    "kr_news['text'] = kr_news['text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "kr_news['text'] = kr_news['text'].apply(lambda x: re.sub(r'\\.{2,}', '.', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c129d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목이 빈 문자열이라서 삭제된 행 :  0\n",
      "본문이 20자 이하라서 작제된 행 :  0\n"
     ]
    }
   ],
   "source": [
    "#제목이 빈 문자열 행 제거\n",
    "removed_empty_title = kr_news[kr_news['title'].astype(str).str.strip() == ''].copy()\n",
    "removed_empty_title[\"reason\"] = \"제목 빈 문자열\"\n",
    "kr_news = kr_news[kr_news['title'].astype(str).str.strip() != '']\n",
    "\n",
    "#본문이 20자 이하인 행 제거\n",
    "removed_short_text = kr_news[kr_news['text'].astype(str).str.len() <= 20].copy()\n",
    "removed_short_text[\"reason\"] = \"본문 20자 이하\"\n",
    "kr_news = kr_news[kr_news['text'].astype(str).str.len() > 20]\n",
    "\n",
    "print(\"제목이 빈 문자열이라서 삭제된 행 : \",len(removed_empty_title))\n",
    "print(\"본문이 20자 이하라서 작제된 행 : \",len(removed_short_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44d64c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스포츠 관련 기사 제거: 115\n"
     ]
    }
   ],
   "source": [
    "#스포츠 종목 및 리그\n",
    "SPORT_KEYWORDS = [\n",
    "    \"야구\", \"농구\", \"축구\", \"골프\", \"e스포츠\", \"LCK\", \"롤드컵\",\n",
    "    \"KBO\", \"KBL\", \"KPGA\", \"프로야구\", \"프로농구\"\n",
    "]\n",
    "\n",
    "#스포츠 뉴스에서 자주 사용되는 단어들\n",
    "SPORT_CONTEXT_KEYWORDS = [\n",
    "    \"경기\", \"시즌\", \"우승\", \"패배\", \"완패\", \"승리\", \"라운드\", \"감독\", \n",
    "    \"리그\", \"순위\", \"스코어\", \"1위\", \"2위\", \"3위\", \"시드\", \"예선\", \"결승\", \n",
    "    \"챔피언\", \"대회\", \"출전\", \"득점\", \"홈런\", \"발로란트\",\"공동\", \"GEN.G\", \n",
    "    \"코치\", \"포스트시즌\", \"정규시즌\", \"완파\", \"역전\", \"프로\", \"진출\", \"연봉\"\n",
    "]\n",
    "\n",
    "\n",
    "def check_sports_reason(title, text):\n",
    "    content = (title + \" \" + text).lower()\n",
    "\n",
    "    found_sports = [k for k in SPORT_KEYWORDS if k.lower() in content]\n",
    "    found_context = [k for k in SPORT_CONTEXT_KEYWORDS if k.lower() in content]\n",
    "    has_business = any(k in content for k in [\"매출\", \"실적\", \"계약\", \"공시\", \"출시\", \"사업\"])\n",
    "\n",
    "    # 스포츠 키워드 존재 + 문맥 키워드 3개 이상 + 비즈니스 문맥 아님\n",
    "    if found_sports and len(found_context) >= 3 and not has_business:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "sports_flags = kr_news.apply(lambda r: check_sports_reason(str(r['title']), str(r['text'])), axis=1)\n",
    "\n",
    "removed_sports = kr_news[sports_flags].copy()\n",
    "removed_sports[\"reason\"] = \"스포츠 뉴스\"\n",
    "kr_news = kr_news[~sports_flags]\n",
    "\n",
    "print(\"스포츠 관련 기사 제거:\", len(removed_sports))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34f10785",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_all = pd.concat([\n",
    "    removed_duplicates,\n",
    "    removed_nan,\n",
    "    removed_pattern,\n",
    "    removed_mismatch,\n",
    "    removed_empty_title,\n",
    "    removed_short_text,\n",
    "    removed_sports\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "388d01b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 새로 추가 중!!\n",
      "전체 뉴스 개수 :  373\n",
      "trash_news.csv로 저장 완료!!\n"
     ]
    }
   ],
   "source": [
    "output_path = 'trash_news.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    df_existing = pd.read_csv(output_path, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"기존 뉴스 개수 : \",len(df_existing))\n",
    "    print(\"추가 뉴스 개수 : \",len(removed_all))\n",
    "    # 새 데이터와 기존 데이터 병합\n",
    "    removed_all = pd.concat([df_existing, removed_all], ignore_index=True)\n",
    "    # 이거는 혹시 크롤링 id가 겹칠 수 있으면 지우는 걸로 하겠습니다.(지금은 테스트 중이라!!!!!!!!!!!!!!!!)\n",
    "    removed_all = removed_all.drop_duplicates(subset=['crawled_news_id'], keep='last')\n",
    "else:\n",
    "    print(\"파일 새로 추가 중!!\")\n",
    "print(\"전체 뉴스 개수 : \",len(removed_all))\n",
    "\n",
    "removed_all['pub_date'] = pd.to_datetime(removed_all['pub_date'], errors='coerce')\n",
    "\n",
    "# 날짜순 정렬 (최신 데이터가 위로)\n",
    "removed_all = removed_all.sort_values('pub_date', ascending=False).reset_index(drop=True)\n",
    "removed_all.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"trash_news.csv로 저장 완료!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "677c9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 새로 추가 중!!\n",
      "전체 뉴스 개수 :  7543\n",
      "news_1st.csv로 저장 완료!!\n"
     ]
    }
   ],
   "source": [
    "output_path = 'news_1st.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    df_existing = pd.read_csv(output_path, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"기존 뉴스 개수 : \",len(df_existing))\n",
    "    print(\"추가 뉴스 개수 : \",len(kr_news))\n",
    "    # 새 데이터와 기존 데이터 병합\n",
    "    kr_news = pd.concat([df_existing, kr_news], ignore_index=True)\n",
    "    # 이거는 혹시 크롤링 id가 겹칠 수 있으면 지우는 걸로 하겠습니다.(지금은 테스트 중이라!!!!!!!!!!!!!!!!)\n",
    "    kr_news = kr_news.drop_duplicates(subset=['crawled_news_id'], keep='last')\n",
    "else:\n",
    "    print(\"파일 새로 추가 중!!\")\n",
    "print(\"전체 뉴스 개수 : \",len(kr_news))\n",
    "\n",
    "kr_news['pub_date'] = pd.to_datetime(kr_news['pub_date'], errors='coerce')\n",
    "\n",
    "# 날짜순 정렬 (최신 데이터가 위로)\n",
    "kr_news = kr_news.sort_values('pub_date', ascending=False).reset_index(drop=True)\n",
    "kr_news.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"news_1st.csv로 저장 완료!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
