import io
from datetime import datetime
import pendulum
import logging

from airflow.decorators import dag, task
from airflow.hooks.base import BaseHook

# ê°€ìƒí™˜ê²½ ë‚´ì˜ íŒŒì´ì¬ ê²½ë¡œ
PYTHON_VENV_PATH = '/opt/airflow/venv_nlp/bin/python'

local_tz = pendulum.timezone("Asia/Seoul")


@dag(
    dag_id='news_refinement',
    schedule_interval=None,
    start_date=datetime(2025, 12, 1, tzinfo=local_tz),
    catchup=False,
    render_template_as_native_obj=True,
    tags=['silver', 'refinement', 'external_python']
)
def news_refinement_pipeline():
    # 1. ì‹¤ì œ ì •ì œ ë¡œì§ (ê°€ìƒí™˜ê²½ ì‹¤í–‰)
    @task.external_python(python=PYTHON_VENV_PATH)
    def refine_task_in_venv(updated_dates, aws_conn_info):
        import io
        import re
        import html
        import ast
        import pandas as pd
        import boto3
        try:
            import hanja
            from kiwipiepy import Kiwi
        except ImportError:
            return "Required libraries not found in venv"

        class NewsPreProcessor:
            def __init__(self):
                self.remove_patterns = [
                    r'\(.*?\)|\[.*?\]|\{.*?\}|<.*?>', r'\S*@\S*', r'http\S+|www\S+',
                    r'\S*=\S*', r'[ê°€-í£]{2,4}\s?(ê¸°ì|íŠ¹íŒŒì›)',
                    r'(ì—°í•©ë‰´ìŠ¤|ë‰´ìŠ¤1|ë‰´ì‹œìŠ¤|ì¡°ì„ ì¼ë³´|ì¤‘ì•™ì¼ë³´|ë™ì•„ì¼ë³´|í•œê²¨ë ˆ|í•œêµ­ì¼ë³´|ì„œìš¸ê²½ì œ|ë§¤ì¼ê²½ì œ|ë¨¸ë‹ˆíˆ¬ë°ì´|í•œêµ­ê²½ì œ|ê²½í–¥ì‹ ë¬¸|í—¤ëŸ´ë“œê²½ì œ|ì•„ì‹œì•„ê²½ì œ|ì´ë°ì¼ë¦¬|ë°ì¼ë¦¬ì•ˆ|ì„¸ê³„ì¼ë³´|êµ­ë¯¼ì¼ë³´|ë‰´ìŠ¤í•Œ|íŒŒì´ë‚¸ì…œë‰´ìŠ¤)',
                    r'(ë¬´ë‹¨ì „ì¬\s*ë°\s*ì¬ë°°í¬\s*ê¸ˆì§€|ì €ì‘ê¶Œì[^.,\n]+|Copyright\s*â“’[^.,\n]+|ë\)|ë$)',
                    r'(ì‚¬ì§„\s*=\s*[^.,\n]+|ê´€ë ¨ê¸°ì‚¬|ì£¼ìš”ë‰´ìŠ¤|ì´ ì‹œê° ë‰´ìŠ¤)[^.\n]*',
                    r'â€».*|â–¶.*|â˜….*'
                ]

            def clean_text_basic(self, text):
                if not isinstance(text, str): return ""
                text = html.unescape(text)
                text = hanja.translate(text, 'substitution')
                for pat in self.remove_patterns:
                    text = re.sub(pat, '', text)
                text = re.sub(r'[^A-Za-z0-9ê°€-í‡.,\"\'\:\Â·\!\?\-\%\~\&]', ' ', text)
                text = re.sub(r'\s+', ' ', text).strip()
                return text

            def is_english_only(self, text):
                sentences = [s.strip() for s in text.split('.') if s.strip()]
                cleaned = [s for s in sentences if re.search(r'[ê°€-í£]', s)]
                return '. '.join(cleaned) + ('.' if cleaned else '')

            def is_sports_news(self, title, text):
                content = (str(title) + " " + str(text)).lower()
                sports_kwd = ["ì•¼êµ¬", "ë†êµ¬", "ì¶•êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "KBO"]
                context_kwd = ["ê²½ê¸°", "ì‹œì¦Œ", "ìš°ìŠ¹", "íŒ¨ë°°", "ë¦¬ê·¸", "ìˆœìœ„", "ìŠ¤ì½”ì–´"]
                has_sports = any(k in content for k in sports_kwd)
                return has_sports and sum(1 for k in context_kwd if k in content) >= 3

        processor = NewsPreProcessor()
        s3_client = boto3.client(
            's3',
            aws_access_key_id=aws_conn_info['access_key'],
            aws_secret_access_key=aws_conn_info['secret_key'],
            endpoint_url=aws_conn_info['endpoint_url']
        )

        if isinstance(updated_dates, str):
            print(f"âš ï¸ updated_datesê°€ ë¬¸ìì—´ë¡œ ë“¤ì–´ì™”ìŠµë‹ˆë‹¤. ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤: {updated_dates}")
            try:
                updated_dates = ast.literal_eval(updated_dates)
            except (ValueError, SyntaxError):
                # ë§Œì•½ '[' ê°€ ì—†ëŠ” ë‹¨ì¼ ë‚ ì§œ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
                updated_dates = [updated_dates]

        for date_str in updated_dates:
            print(f"ğŸ” Processing date string: '{date_str}' (type: {type(date_str)})")

            if not date_str or not isinstance(date_str, str) or '-' not in date_str:
                print(f"âš ï¸ Skip invalid date format: {date_str}")
                continue

            dt = date_str.split('-')

            if len(dt) < 3:
                print(f"âš ï¸ Unexpected date format (split count {len(dt)}): {date_str}")
                continue

            year, month, day = dt[0], dt[1], dt[2]
            object_key = f'crawled-news/year={year}/month={month}/day={day}/data.parquet'

            try:
                response = s3_client.get_object(Bucket='bronze', Key=object_key)
                df = pd.read_parquet(io.BytesIO(response['Body'].read()))

                trash_dfs = []
                df_initial = df.copy()
                df = df.drop_duplicates(subset=['text']).dropna(subset=['text'])

                # ì¤‘ë³µ ë°ì´í„° ì²´í¬ ë¡œì§ ìˆ˜ì • (ID ê¸°ì¤€)
                if len(df_initial) != len(df):
                    removed_mask = ~df_initial['crawled_news_id'].isin(df['crawled_news_id'])
                    df_trash_dup = df_initial[removed_mask].copy()
                    df_trash_dup['reason'] = 'duplicate_or_nan'
                    trash_dfs.append(df_trash_dup)

                df['text'] = df['text'].apply(processor.clean_text_basic)
                df['text'] = df['text'].apply(processor.is_english_only)

                mask_sports = df.apply(lambda r: processor.is_sports_news(r.get('title', ''), r['text']), axis=1)
                mask_short = df['text'].str.len() <= 20

                if mask_sports.any():
                    df_trash_sports = df[mask_sports].copy()
                    df_trash_sports['reason'] = 'sports_news'
                    trash_dfs.append(df_trash_sports)

                if mask_short.any():
                    df_trash_short = df[mask_short].copy()
                    df_trash_short['reason'] = 'short_text'
                    trash_dfs.append(df_trash_short)

                df_final = df[~(mask_sports | mask_short)].copy()

                # Silver ì €ì¥
                silver_buffer = io.BytesIO()
                df_final.to_parquet(silver_buffer, engine='pyarrow', index=False)
                s3_client.put_object(Bucket='silver', Key=object_key, Body=silver_buffer.getvalue())

                # Trash ì €ì¥
                if trash_dfs:
                    df_trash_total = pd.concat(trash_dfs, ignore_index=True)
                    trash_buffer = io.BytesIO()
                    df_trash_total.to_parquet(trash_buffer, engine='pyarrow', index=False)

                    # [ìˆ˜ì • í¬ì¸íŠ¸] ê²½ë¡œ ì•ì— 'trash/'ë¥¼ ë¶™ì—¬ì„œ ì¤‘ë³µì„ ë°©ì§€í•©ë‹ˆë‹¤.
                    trash_key = f"trash/{object_key}"

                    s3_client.put_object(
                        Bucket='silver',
                        Key=trash_key,  # ìˆ˜ì •ëœ ê²½ë¡œ: trash/crawled-news/year=...
                        Body=trash_buffer.getvalue()
                    )
                    trash_count = len(df_trash_total)
                else:
                    trash_count = 0

                print(f"âœ… {date_str}: Silver({len(df_final)}), Trash({len(df_trash_total) if trash_dfs else 0})")

            except Exception as e:
                print(f"âŒ Error processing {date_str}: {e}")

        return f"Successfully processed: {updated_dates}"

    # 2. íŒŒë¼ë¯¸í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” íƒœìŠ¤í¬ (Airflow ë©”ì¸ í™˜ê²½ ì‹¤í–‰)
    @task(multiple_outputs=True)
    def prepare_params(**context):
        conf = context['dag_run'].conf
        dates = conf.get('updated_dates', [])

        # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ê°•ì œ ì¢…ë£Œí•˜ê±°ë‚˜ ë¡œê¹…
        if not dates:
            logging.warning("âš ï¸ No updated_dates found in dag_run.conf")

        conn = BaseHook.get_connection('MINIO_S3')
        aws_info = {
            "access_key": conn.login,
            "secret_key": conn.password,
            "endpoint_url": conn.extra_dejson.get('endpoint_url')
        }

        # ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜ (ë‹¤ìŒ íƒœìŠ¤í¬ë¡œ ì „ë‹¬ë¨)
        return {"dates": dates, "aws": aws_info}

    # --- DAG êµ¬ì¡° ì •ì˜ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„!) ---
    params = prepare_params()

    # paramsì—ì„œ í•„ìš”í•œ ê°’ì„ êº¼ë‚´ì–´ refine_task_in_venvë¡œ ì „ë‹¬
    # refine_task_in_venvë¥¼ ì—¬ê¸°ì„œ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ TaskFlow APIì˜ ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•ì…ë‹ˆë‹¤.
    refine_task_in_venv(
        updated_dates=params['dates'],
        aws_conn_info=params['aws']
    )


news_refinement_dag = news_refinement_pipeline()