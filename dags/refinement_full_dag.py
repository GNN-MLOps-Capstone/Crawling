import io
import re
import html
import logging
from datetime import datetime
import pendulum

from airflow.decorators import dag, task
from airflow.hooks.base import BaseHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# ê°€ìƒí™˜ê²½ ë‚´ì˜ íŒŒì´ì¬ ê²½ë¡œ
PYTHON_VENV_PATH = '/opt/airflow/venv_nlp/bin/python'
local_tz = pendulum.timezone("Asia/Seoul")


@dag(
    dag_id='news_refinement_full_refresh',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ ì „ìš©
    start_date=datetime(2025, 12, 1, tzinfo=local_tz),
    catchup=False,
    tags=['silver', 'full-refresh', 'external_python']
)
def news_refinement_full_pipeline():
    # 1. Bronze ë²„í‚·ì˜ ëª¨ë“  ë‚ ì§œ íŒŒí‹°ì…˜ì„ ë¦¬ìŠ¤íŠ¸ì—…í•˜ëŠ” íƒœìŠ¤í¬
    @task
    def list_all_bronze_dates():
        s3_hook = S3Hook(aws_conn_id='MINIO_S3')
        bucket_name = 'bronze'
        prefix = 'crawled-news/'

        # ëª¨ë“  parquet íŒŒì¼ í‚¤ ê°€ì ¸ì˜¤ê¸°
        keys = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix)
        if not keys:
            return []

        # ê²½ë¡œì—ì„œ ë‚ ì§œ ì •ë³´(YYYY-MM-DD) ì¶”ì¶œ
        # ì˜ˆ: crawled-news/year=2025/month=12/day=22/data.parquet
        date_pattern = re.compile(r'year=(\d{4})/month=(\d{2})/day=(\d{2})')
        dates = set()
        for key in keys:
            match = date_pattern.search(key)
            if match:
                dates.add(f"{match.group(1)}-{match.group(2)}-{match.group(3)}")

        sorted_dates = sorted(list(dates))
        logging.info(f"ğŸ” ì´ {len(sorted_dates)}ê°œì˜ ë‚ ì§œ íŒŒí‹°ì…˜ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return sorted_dates

    # 2. AWS ì—°ê²° ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” íƒœìŠ¤í¬
    @task
    def get_aws_info():
        conn = BaseHook.get_connection('MINIO_S3')
        return {
            "access_key": conn.login,
            "secret_key": conn.password,
            "endpoint_url": conn.extra_dejson.get('endpoint_url')
        }

    # 3. ê°œë³„ ë‚ ì§œì— ëŒ€í•´ ì •ì œë¥¼ ìˆ˜í–‰í•˜ëŠ” íƒœìŠ¤í¬ (ê°€ìƒí™˜ê²½)
    @task.external_python(python=PYTHON_VENV_PATH)
    def refine_single_date_in_venv(date_str, aws_conn_info):
        import io
        import re
        import html
        import pandas as pd
        import boto3
        try:
            import hanja
            from kiwipiepy import Kiwi
        except ImportError:
            raise ImportError("Required libraries not found in venv")

        # --- ì •ì œ ë¡œì§ í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼) ---
        class NewsPreProcessor:
            def __init__(self):
                self.remove_patterns = [
                    r'\(.*?\)|\[.*?\]|\{.*?\}|<.*?>', r'\S*@\S*', r'http\S+|www\S+',
                    r'\S*=\S*', r'[ê°€-í£]{2,4}\s?(ê¸°ì|íŠ¹íŒŒì›)',
                    r'(ì—°í•©ë‰´ìŠ¤|ë‰´ìŠ¤1|ë‰´ì‹œìŠ¤|ì¡°ì„ ì¼ë³´|ì¤‘ì•™ì¼ë³´|ë™ì•„ì¼ë³´|í•œê²¨ë ˆ|í•œêµ­ì¼ë³´|ì„œìš¸ê²½ì œ|ë§¤ì¼ê²½ì œ|ë¨¸ë‹ˆíˆ¬ë°ì´|í•œêµ­ê²½ì œ|ê²½í–¥ì‹ ë¬¸|í—¤ëŸ´ë“œê²½ì œ|ì•„ì‹œì•„ê²½ì œ|ì´ë°ì¼ë¦¬|ë°ì¼ë¦¬ì•ˆ|ì„¸ê³„ì¼ë³´|êµ­ë¯¼ì¼ë³´|ë‰´ìŠ¤í•Œ|íŒŒì´ë‚¸ì…œë‰´ìŠ¤)',
                    r'(ë¬´ë‹¨ì „ì¬\s*ë°\s*ì¬ë°°í¬\s*ê¸ˆì§€|ì €ì‘ê¶Œì[^.,\n]+|Copyright\s*â“’[^.,\n]+|ë\)|ë$)',
                    r'(ì‚¬ì§„\s*=\s*[^.,\n]+|ê´€ë ¨ê¸°ì‚¬|ì£¼ìš”ë‰´ìŠ¤|ì´ ì‹œê° ë‰´ìŠ¤)[^.\n]*',
                    r'â€».*|â–¶.*|â˜….*'
                ]

            def clean_text_basic(self, text):
                if not isinstance(text, str): return ""
                text = html.unescape(text)
                text = hanja.translate(text, 'substitution')
                for pat in self.remove_patterns:
                    text = re.sub(pat, '', text)
                text = re.sub(r'[^A-Za-z0-9ê°€-í‡.,\"\'\:\Â·\!\?\-\%\~\&]', ' ', text)
                text = re.sub(r'\s+', ' ', text).strip()
                return text

            def is_english_only(self, text):
                sentences = [s.strip() for s in text.split('.') if s.strip()]
                cleaned = [s for s in sentences if re.search(r'[ê°€-í£]', s)]
                return '. '.join(cleaned) + ('.' if cleaned else '')

            def is_sports_news(self, title, text):
                content = (str(title) + " " + str(text)).lower()
                sports_kwd = ["ì•¼êµ¬", "ë†êµ¬", "ì¶•êµ¬", "ê³¨í”„", "eìŠ¤í¬ì¸ ", "KBO"]
                context_kwd = ["ê²½ê¸°", "ì‹œì¦Œ", "ìš°ìŠ¹", "íŒ¨ë°°", "ë¦¬ê·¸", "ìˆœìœ„", "ìŠ¤ì½”ì–´"]
                has_sports = any(k in content for k in sports_kwd)
                return has_sports and sum(1 for k in context_kwd if k in content) >= 3

        processor = NewsPreProcessor()
        s3_client = boto3.client(
            's3',
            aws_access_key_id=aws_conn_info['access_key'],
            aws_secret_access_key=aws_conn_info['secret_key'],
            endpoint_url=aws_conn_info['endpoint_url']
        )

        # ê²½ë¡œ ì„¤ì •
        dt = date_str.split('-')
        year, month, day = dt[0], dt[1], dt[2]
        object_key = f'crawled-news/year={year}/month={month}/day={day}/data.parquet'

        print(f"ğŸš€ Processing: {object_key}")

        # ë°ì´í„° ë¡œë“œ
        response = s3_client.get_object(Bucket='bronze', Key=object_key)
        df = pd.read_parquet(io.BytesIO(response['Body'].read()))

        trash_dfs = []
        df_initial = df.copy()
        df = df.drop_duplicates(subset=['text']).dropna(subset=['text'])

        # ì¤‘ë³µ ì œê±° ë¡œê·¸ ë° Trash ìˆ˜ì§‘
        if len(df_initial) != len(df):
            removed_mask = ~df_initial['crawled_news_id'].isin(df['crawled_news_id'])
            df_trash_dup = df_initial[removed_mask].copy()
            df_trash_dup['reason'] = 'duplicate_or_nan'
            trash_dfs.append(df_trash_dup)

        # ì •ì œ ìˆ˜í–‰
        df['text'] = df['text'].apply(processor.clean_text_basic)
        df['text'] = df['text'].apply(processor.is_english_only)

        mask_sports = df.apply(lambda r: processor.is_sports_news(r.get('title', ''), r['text']), axis=1)
        mask_short = df['text'].str.len() <= 20

        # Trash ë¶„ë¦¬
        if mask_sports.any():
            df_trash_sports = df[mask_sports].copy()
            df_trash_sports['reason'] = 'sports_news'
            trash_dfs.append(df_trash_sports)
        if mask_short.any():
            df_trash_short = df[mask_short].copy()
            df_trash_short['reason'] = 'short_text'
            trash_dfs.append(df_trash_short)

        df_final = df[~(mask_sports | mask_short)].copy()

        # [ì €ì¥ 1] Silver ë°ì´í„°
        silver_buffer = io.BytesIO()
        df_final.to_parquet(silver_buffer, engine='pyarrow', index=False)
        s3_client.put_object(Bucket='silver', Key=object_key, Body=silver_buffer.getvalue())

        # [ì €ì¥ 2] Trash ë°ì´í„° (ê²½ë¡œ ë¶„ë¦¬)
        if trash_dfs:
            df_trash_total = pd.concat(trash_dfs, ignore_index=True)
            trash_buffer = io.BytesIO()
            df_trash_total.to_parquet(trash_buffer, engine='pyarrow', index=False)
            s3_client.put_object(
                Bucket='silver',
                Key=f"trash/{object_key}",
                Body=trash_buffer.getvalue()
            )
            trash_count = len(df_trash_total)
        else:
            trash_count = 0

        print(f"âœ… {date_str} ì²˜ë¦¬ ì™„ë£Œ: Silver {len(df_final)}ê±´, Trash {trash_count}ê±´")
        return f"{date_str} success"

    # --- ì‹¤ì‹œê°„ íƒœìŠ¤í¬ ë§¤í•‘ ì‹¤í–‰ ---
    all_dates = list_all_bronze_dates()
    aws_info = get_aws_info()

    # .expandë¥¼ ì‚¬ìš©í•˜ë©´ all_datesì˜ ë¦¬ìŠ¤íŠ¸ ê°œìˆ˜ë§Œí¼ íƒœìŠ¤í¬ê°€ ë³‘ë ¬ë¡œ ìë™ ìƒì„±ë©ë‹ˆë‹¤.
    refine_single_date_in_venv.expand(date_str=all_dates, aws_conn_info=[aws_info])


news_refinement_full_refresh_dag = news_refinement_full_pipeline()