import io
import pandas as pd
from airflow.providers.amazon.aws.hooks.s3 import S3Hook


def write_news_to_minio(df: pd.DataFrame, bucket_name: str, conn_id: str) -> list:
    """
    [Target: MinIO]
    DataFrameì„ ë‚ ì§œë³„ íŒŒí‹°ì…˜(year/month/day)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì ì¬(Upsert)í•©ë‹ˆë‹¤.
    """
    if df.empty:
        return []

    s3_hook = S3Hook(aws_conn_id=conn_id)

    # íŒŒí‹°ì…”ë‹ í‚¤ ìƒì„±
    df['pub_date'] = pd.to_datetime(df['pub_date'])
    df['date_key'] = df['pub_date'].dt.date

    uploaded_paths = []

    for date_key in df['date_key'].unique():
        partition_df = df[df['date_key'] == date_key].copy()

        # Hive Style Partition Path ìƒì„±
        y, m, d = date_key.strftime('%Y'), date_key.strftime('%m'), date_key.strftime('%d')
        object_key = f'crawled_news/year={y}/month={m}/day={d}/data.parquet'

        # [Idempotency] ê¸°ì¡´ ë°ì´í„° ë³‘í•© ë¡œì§
        if s3_hook.check_for_key(object_key, bucket_name):
            print(f"ğŸ”„ [Writer] Merging with existing: {object_key}")
            try:
                obj = s3_hook.get_key(object_key, bucket_name)
                existing_df = pd.read_parquet(io.BytesIO(obj.get()['Body'].read()))

                # ë³‘í•© ë° ì¤‘ë³µ ì œê±°
                combined_df = pd.concat([existing_df, partition_df])
                final_df = combined_df.drop_duplicates(subset=['news_id'], keep='last')
            except Exception as e:
                print(f"âš ï¸ [Writer] Read Error (Overwrite): {e}")
                final_df = partition_df
        else:
            final_df = partition_df

        # ì„ì‹œ ì»¬ëŸ¼ ì •ë¦¬
        if 'date_key' in final_df.columns:
            final_df = final_df.drop(columns=['date_key'])

        # Upload
        out_buffer = io.BytesIO()
        final_df.to_parquet(out_buffer, engine='pyarrow', index=False)

        s3_hook.load_file_obj(
            file_obj=io.BytesIO(out_buffer.getvalue()),
            key=object_key,
            bucket_name=bucket_name,
            replace=True
        )
        uploaded_paths.append(object_key)
        print(f"âœ… [Writer] Saved: {object_key} ({len(final_df)} rows)")

    return uploaded_paths