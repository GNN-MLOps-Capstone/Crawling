import io
import pandas as pd
import boto3
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ Import
from modules.analysis.gemini_extractor import GeminiExtractor
from modules.analysis.gemini_sentiment import SentimentAnalyzer

def run_stage1_extraction(updated_dates: list, stock_map: dict, aws_info: dict, api_key: str) -> list:
    """[Service] 1ë‹¨ê³„: ë‰´ìŠ¤ ê¸°ë³¸ ì •ë³´(ì¢…ëª©, í‚¤ì›Œë“œ, ìš”ì•½, ì „ì²´ê°ì„±) ì¶”ì¶œ"""
    s3 = boto3.client('s3', aws_access_key_id=aws_info['access_key'], aws_secret_access_key=aws_info['secret_key'],
                      endpoint_url=aws_info['endpoint_url'])
    processed_keys = []
    extractor = GeminiExtractor(stock_map, api_key)
    MAX_WORKERS = 8 # API ë“±ê¸‰ì— ë§ì¶° ì¡°ì ˆ

    for date_str in updated_dates:
        print(f"ğŸ” [Stage 1] Processing: {date_str}")
        y, m, d = date_str.split('-')
        input_key = f'refined_news/year={y}/month={m}/day={d}/data.parquet'
        # 1ì°¨ ë¶„ì„ ê²°ê³¼ ì„ì‹œ ì €ì¥ ê²½ë¡œ
        output_key = f'extracted_stage1/year={y}/month={m}/day={d}/data.parquet'

        try:
            # Load Data
            obj = s3.get_object(Bucket='silver', Key=input_key)
            df = pd.read_parquet(io.BytesIO(obj['Body'].read()))

            print(f"ğŸš€ {date_str} ë°ì´í„° 1ë‹¨ê³„ ë¶„ì„ ì¤‘... (ì´ {len(df)}ê±´)")
            results = {}
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_idx = {
                    executor.submit(extractor.extract, row.get('refined_text', row.get('text', '')), row['news_id']): idx 
                    for idx, row in df.iterrows()
                }
                for future in tqdm(as_completed(future_to_idx), total=len(df), desc="Stage 1 Extraction"):
                    idx = future_to_idx[future]
                    results[idx] = future.result()

            # Merge Results
            for idx, res in results.items():
                df.at[idx, 'related_stocks'] = res['related_stocks']
                df.at[idx, 'keywords'] = res['keywords']
                df.at[idx, 'overall_sentiment'] = res['sentiment']
                df.at[idx, 'summary'] = res['summary']

            # Save to S3
            out_buf = io.BytesIO()
            df.to_parquet(out_buf, index=False)
            s3.put_object(Bucket='silver', Key=output_key, Body=out_buf.getvalue())
            print(f"âœ… Stage 1 Saved: {output_key}")
            processed_keys.append(output_key)

        except s3.exceptions.NoSuchKey:
            print(f"âš ï¸ Data not found: {input_key}")
        except Exception as e:
            print(f"âŒ Error in Stage 1: {e}")

    return processed_keys


def run_stage2_sentiment(updated_dates: list, aws_info: dict, api_key: str) -> list:
    """[Service] 2ë‹¨ê³„: ê´€ë ¨ ì£¼ì‹ì´ ìˆëŠ” ê¸°ì‚¬ ëŒ€ìƒ ìƒì„¸ ê°ì„± ë¶„ì„"""
    s3 = boto3.client('s3', aws_access_key_id=aws_info['access_key'], aws_secret_access_key=aws_info['secret_key'],
                      endpoint_url=aws_info['endpoint_url'])
    processed_keys = []
    analyzer = SentimentAnalyzer(api_key)
    MAX_WORKERS = 3 # 2ì°¨ ë¶„ì„ì€ í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ì–´ ë¶€í•˜ê°€ í´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì›Œì»¤ ìˆ˜ë¥¼ ì¤„ì„

    for date_str in updated_dates:
        print(f"ğŸ” [Stage 2] Processing: {date_str}")
        y, m, d = date_str.split('-')
        # 1ì°¨ ë¶„ì„ì´ ëë‚œ ë°ì´í„°ë¥¼ Inputìœ¼ë¡œ ì‚¬ìš©
        input_key = f'extracted_stage1/year={y}/month={m}/day={d}/data.parquet'
        # ìµœì¢… ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        output_key = f'extracted_final/year={y}/month={m}/day={d}/data.parquet'

        try:
            # Load Data
            obj = s3.get_object(Bucket='silver', Key=input_key)
            df = pd.read_parquet(io.BytesIO(obj['Body'].read()))

            # ë¶„ì„ ëŒ€ìƒ í•„í„°ë§: related_stocks ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ë§Œ ì„ íƒ
            target_df = df[df['related_stocks'].apply(lambda x: isinstance(x, list) and len(x) > 0)]
            
            print(f"ğŸ”¬ {date_str} ë°ì´í„° 2ë‹¨ê³„ ë¶„ì„ ì¤‘... (ëŒ€ìƒ {len(target_df)}ê±´)")
            results = {}
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_idx = {
                    executor.submit(
                        analyzer.analyze, 
                        row.get('refined_text', row.get('text', '')), 
                        row['related_stocks'], 
                        row['keywords']
                    ): idx 
                    for idx, row in target_df.iterrows()
                }
                for future in tqdm(as_completed(future_to_idx), total=len(target_df), desc="Stage 2 Sentiment"):
                    idx = future_to_idx[future]
                    results[idx] = future.result()

            # Merge Results (ì „ì²´ dfì— ë³‘í•©)
            df['related_stocks_sentiment'] = None
            df['keywords_sentiment'] = None
            for idx, res in results.items():
                df.at[idx, 'related_stocks_sentiment'] = res['stock_sentiments']
                df.at[idx, 'keywords_sentiment'] = res['keyword_sentiments']

            # Save Final Data to S3
            out_buf = io.BytesIO()
            df.to_parquet(out_buf, index=False)
            s3.put_object(Bucket='silver', Key=output_key, Body=out_buf.getvalue())
            print(f"âœ… Final Data Saved: {output_key}")
            processed_keys.append(output_key)

        except s3.exceptions.NoSuchKey:
            print(f"âš ï¸ Stage 1 Data not found: {input_key}")
        except Exception as e:
            print(f"âŒ Error in Stage 2: {e}")

    return processed_keys