import io
import pandas as pd
import boto3
import pyarrow as pa
import pyarrow.parquet as pq
import hashlib
import psycopg2
from psycopg2.extras import execute_values  # ÌïÑÏàò

from modules.analysis.preprocessor import NewsPreProcessor
from modules.analysis.news_embedding import add_embeddings_to_df


def run_refinement_process(updated_dates: list, aws_info: dict, db_info: dict):
    s3_client = boto3.client('s3',
                             aws_access_key_id=aws_info['access_key'],
                             aws_secret_access_key=aws_info['secret_key'],
                             endpoint_url=aws_info['endpoint_url'])

    OLLAMA_HOST = "http://ollama:11434"
    processed_files = []

    def generate_content_hash(text):
        if not isinstance(text, str): return ''
        clean_content = text.replace(' ', '').replace('\n', '').replace('\r', '').replace('\t', '')
        return hashlib.md5(clean_content.encode('utf-8')).hexdigest()

    for date_str in updated_dates:
        print(f"üîé Processing date: {date_str}")
        if not date_str or '-' not in date_str: continue

        y, m, d = date_str.split('-')
        input_key = f'crawled_news/year={y}/month={m}/day={d}/data.parquet'
        output_key = f'refined_news/year={y}/month={m}/day={d}/data.parquet'

        try:
            # 1. Load (Bronze)
            try:
                response = s3_client.get_object(Bucket='bronze', Key=input_key)
                df = pd.read_parquet(io.BytesIO(response['Body'].read()))
            except s3_client.exceptions.NoSuchKey:
                print(f"‚ö†Ô∏è Input data not found: {input_key}")
                continue

            if df.empty: continue

            print(f"  - Raw count: {len(df)}")
            df = df.dropna(subset=['text'])
            df['content_hash'] = df['text'].apply(generate_content_hash)

            # ÌååÏùº ÎÇ¥Î∂Ä Ï§ëÎ≥µ Ï†úÍ±∞
            df = df.drop_duplicates(subset=['content_hash'], keep='first')

            if df.empty: continue

            # -----------------------------------------------------------
            # [ÏàòÏ†ïÎê®] Î™ÖÏãúÏ†Å Ï§ëÎ≥µ Í≤ÄÏÇ¨ Î°úÏßÅ (Select -> Insert)
            # -----------------------------------------------------------
            unique_hashes = df['content_hash'].unique().tolist()
            target_hashes = set(unique_hashes)

            try:
                conn = psycopg2.connect(**db_info)
                cur = conn.cursor()

                # [ÎîîÎ≤ÑÍπÖÏö©] Ïã§Ï†ú ÌÖåÏù¥Î∏îÏóê Îç∞Ïù¥ÌÑ∞Í∞Ä Î™á Í∞úÎÇò ÏûàÎäîÏßÄ ÌôïÏù∏
                cur.execute("SELECT count(*) FROM processed_content_hashes")
                total_rows = cur.fetchone()[0]
                print(f"  üëÄ [Debug] Total rows in DB table: {total_rows}")

                # 1. DBÏóê Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Ìï¥Ïãú Ï°∞Ìöå (SELECT)
                #    WHERE IN Ï†àÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌòÑÏû¨ Î∞∞ÏπòÏùò Ìï¥Ïãú Ï§ë DBÏóê ÏûàÎäî Í≤ÉÏùÑ Ï∞æÏùå
                if len(unique_hashes) > 0:
                    query = "SELECT content_hash FROM processed_content_hashes WHERE content_hash IN %s"
                    cur.execute(query, (tuple(unique_hashes),))
                    existing_rows = cur.fetchall()
                    existing_hashes = {row[0] for row in existing_rows}
                else:
                    existing_hashes = set()

                print(f"  - Found {len(existing_hashes)} duplicates in DB.")

                # 2. ÏÉàÎ°úÏö¥ Ìï¥ÏãúÎßå ÌïÑÌÑ∞ÎßÅ (Python Ïó∞ÏÇ∞)
                new_hashes_to_process = target_hashes - existing_hashes

                # 3. ÏÉàÎ°úÏö¥ Ìï¥ÏãúÎ•º DBÏóê Îì±Î°ù (INSERT)
                if new_hashes_to_process:
                    insert_query = """
                                   INSERT INTO processed_content_hashes (content_hash)
                                   VALUES %s ON CONFLICT (content_hash) DO NOTHING \
                                   """
                    values = [(h,) for h in new_hashes_to_process]
                    execute_values(cur, insert_query, values)
                    conn.commit()  # Ïª§Î∞ã ÌïÑÏàò
                    print(f"  - Registered {len(new_hashes_to_process)} new hashes to DB.")

                conn.close()

                # 4. DataFrame ÌïÑÌÑ∞ÎßÅ
                initial_count = len(df)
                df = df[df['content_hash'].isin(new_hashes_to_process)].copy()

                dropped_count = initial_count - len(df)
                if dropped_count > 0:
                    print(f"  üî• {dropped_count} duplicates skipped (Already processed).")

            except Exception as e:
                print(f"  ‚ùå DB Deduplication Error: {e}")
                raise e

            if df.empty:
                print("  üí§ All data in this batch has been processed before. Skipping.")
                continue
            # -----------------------------------------------------------

            # 3. Preprocess
            processor = NewsPreProcessor()
            df['refined_text'] = df['text'].apply(processor.clean_text_basic)
            df['refined_text'] = df['refined_text'].apply(processor.is_english_only)

            mask_sports = df['refined_text'].apply(processor.is_sports_news)
            mask_short = df['refined_text'].str.len() <= 20
            df_final = df[~(mask_sports | mask_short)].copy()

            if df_final.empty:
                print("  ‚ö†Ô∏è No valid data after filtering.")
                continue

            # 4. Embedding
            df_embedded = add_embeddings_to_df(df_final, model_name="bge-m3", host=OLLAMA_HOST)

            # 5. Save
            out_cols = ['news_id', 'refined_text', 'news_embedding', 'pub_date']
            df_save = df_embedded[out_cols].copy()
            df_save['news_id'] = df_save['news_id'].astype('int64')
            df_save['pub_date'] = df_save['pub_date'].astype(str)

            arrow_schema = pa.schema([
                ('news_id', pa.int64()),
                ('refined_text', pa.string()),
                ('news_embedding', pa.list_(pa.float32())),
                ('pub_date', pa.string())
            ])

            out_buf = io.BytesIO()
            table = pa.Table.from_pandas(df_save, schema=arrow_schema)
            pq.write_table(table, out_buf, compression='SNAPPY')

            s3_client.put_object(Bucket='silver', Key=output_key, Body=out_buf.getvalue())
            print(f"‚úÖ Saved: {output_key} ({len(df_save)} rows)")
            processed_files.append(output_key)

        except Exception as e:
            print(f"‚ùå Error processing {date_str}: {e}")
            raise e

    return processed_files