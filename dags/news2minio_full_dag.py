import io
import pandas as pd
from datetime import datetime, timedelta
import pendulum

from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# íƒ€ì„ì¡´ ì„¤ì •
local_tz = pendulum.timezone("Asia/Seoul")

# ìƒìˆ˜ ì„¤ì •
TARGET_MINIO_BUCKET = 'bronze'
POSTGRES_CONN_ID = 'news_data_db'
MINIO_CONN_ID = 'MINIO_S3'

default_args = {
    'owner': 'dongbin',
    'retries': 1,  # ì „ì²´ ë™ê¸°í™”ëŠ” ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ê°œì…ì´ ë‚˜ìœ¼ë¯€ë¡œ ë¦¬íŠ¸ë¼ì´ ì¶•ì†Œ
    'retry_delay': timedelta(minutes=5),
}


@dag(
    dag_id='news_db_to_minio_full_sync',  # ID ë³€ê²½ìœ¼ë¡œ ê¸°ì¡´ ë°ì¼ë¦¬ DAGì™€ êµ¬ë¶„
    default_args=default_args,
    start_date=datetime(2025, 12, 1, tzinfo=local_tz),
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ ì „ìš©
    catchup=False,
    tags=['bronze', 'news', 'full_sync']
)
def news_db_to_minio_full_sync():
    @task(task_id='upload_full_data_to_minio')
    def upload_full_data_to_minio():
        """
        PostgreSQLì˜ ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ
        ë°œí–‰ì¼(pub_date) ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…”ë‹í•˜ì—¬ MinIOì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        """
        pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
        s3_hook = S3Hook(aws_conn_id=MINIO_CONN_ID)

        # 1. ì „ì²´ ë°ì´í„° ì¿¼ë¦¬ (ì‹œê°„ ì œí•œ ì¡°ê±´ ì œê±°)
        print("ğŸš€ DBì—ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ë°ì´í„° ì–‘ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        query = """
                SELECT cn.*, nn.pub_date
                FROM public.crawled_news cn
                         JOIN public.naver_news nn ON cn.news_id = nn.news_id \
                """
        all_df = pg_hook.get_pandas_df(query)

        if all_df.empty:
            print("âš ï¸ DBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "No data found"

        print(f"âœ… ì´ {len(all_df)} ê±´ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. íŒŒí‹°ì…”ë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # 2. ë‚ ì§œë³„ íŒŒí‹°ì…”ë‹ ì¤€ë¹„
        all_df['pub_date'] = pd.to_datetime(all_df['pub_date'])
        all_df['date_key'] = all_df['pub_date'].dt.date

        # 3. ë‚ ì§œë³„ ë£¨í”„ ì‹¤í–‰
        unique_dates = sorted(all_df['date_key'].unique())
        print(f"ğŸ“… ì´ {len(unique_dates)} ê°œì˜ ë‚ ì§œ íŒŒí‹°ì…˜ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        for target_date in unique_dates:
            daily_df = all_df[all_df['date_key'] == target_date].copy()

            year = target_date.strftime('%Y')
            month = target_date.strftime('%m')
            day = target_date.strftime('%d')
            object_key = f'crawled-news/year={year}/month={month}/day={day}/data.parquet'

            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ì‹ ê·œ ìƒì„± (Idempotency ìœ ì§€)
            if s3_hook.check_for_key(object_key, TARGET_MINIO_BUCKET):
                print(f"ğŸ”„ ê¸°ì¡´ ë°ì´í„° ë³‘í•© ì¤‘: {object_key}")
                existing_obj = s3_hook.get_key(object_key, TARGET_MINIO_BUCKET)
                existing_df = pd.read_parquet(io.BytesIO(existing_obj.get()['Body'].read()))

                # ì¤‘ë³µ ì œê±° (ê°€ì¥ ìµœì‹  ìˆ˜ì§‘ë³¸ ìœ ì§€)
                combined_df = pd.concat([existing_df, daily_df])
                final_df = combined_df.sort_values('updated_at').drop_duplicates('news_id', keep='last')
            else:
                final_df = daily_df

            # 4. Parquet ë³€í™˜ ë° ì—…ë¡œë“œ
            buffer = io.BytesIO()
            final_df.to_parquet(buffer, engine='pyarrow', index=False)

            s3_hook.load_file_obj(
                file_obj=io.BytesIO(buffer.getvalue()),
                key=object_key,
                bucket_name=TARGET_MINIO_BUCKET,
                replace=True
            )
            print(f"âœ”ï¸ {target_date} ì™„ë£Œ ({len(final_df)} ê±´)")

        print("ğŸŠ ì „ì²´ ë°ì´í„° ë™ê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    upload_full_data_to_minio()


# DAG ì‹¤í–‰
news_full_sync_dag = news_db_to_minio_full_sync()