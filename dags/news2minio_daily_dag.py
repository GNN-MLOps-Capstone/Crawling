import pendulum
from datetime import datetime, timedelta

from airflow.decorators import dag, task
from airflow.datasets import Dataset

# ğŸŸ¢ [Module Import] êµ¬ì¡°í™”ëœ ëª¨ë“ˆ ê²½ë¡œ ì‚¬ìš©
from modules.ingestion.reader import read_daily_news
from modules.ingestion.writer import write_news_to_minio

# [ì„¤ì •]
local_tz = pendulum.timezone("Asia/Seoul")
POSTGRES_CONN_ID = 'news_data_db'
MINIO_CONN_ID = 'MINIO_S3'
TARGET_BUCKET = 'bronze'

# [Dataset] ì´ íŒŒì´í”„ë¼ì¸ì´ ì™„ë£Œë˜ë©´ downstream DAGê°€ ì´ë¥¼ ê°ì§€í•¨
BRONZE_DATASET = Dataset(f"s3://{TARGET_BUCKET}/crawled_news")

default_args = {
    'owner': 'dongbin',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


@dag(
    dag_id='news2minio_daily_v2',  # ID ì—…ë°ì´íŠ¸
    default_args=default_args,
    start_date=datetime(2025, 12, 1, tzinfo=local_tz),
    schedule_interval='0 2 * * *',
    catchup=False,
    tags=['bronze', 'ingestion', 'structured']
)
def news_ingestion_pipeline():
    @task(outlets=[BRONZE_DATASET])
    def ingest_process(**context):
        # 1. Target Date ê³„ì‚° (Yesterday)
        exec_date = context['logical_date'].in_timezone(local_tz)
        target_date = exec_date.subtract(days=1).to_date_string()
        print(f"ğŸš€ [Ingestion] Start processing for: {target_date}")

        # 2. Extract (From Postgres)
        news_df = read_daily_news(target_date, POSTGRES_CONN_ID)

        if news_df.empty:
            print("ğŸ’¤ No data to process. Skipping upload.")
            return []

        # 3. Load (To MinIO)
        saved_files = write_news_to_minio(news_df, TARGET_BUCKET, MINIO_CONN_ID)

        return saved_files

    ingest_process()


dag_instance = news_ingestion_pipeline()